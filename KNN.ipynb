{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "KNN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LmSNbYh44rN2",
        "colab_type": "text"
      },
      "source": [
        "***Import Required Libraries***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYyrnLZ5M6RY",
        "colab_type": "code",
        "outputId": "47a78cc8-5457-4f65-db08-90f0a7fa6cc2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 79
        }
      },
      "source": [
        "import numpy as np\n",
        "import struct\n",
        "from sklearn.decomposition import PCA\n",
        "from keras.datasets import mnist"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kqm0Hq8Ty1O5",
        "colab_type": "text"
      },
      "source": [
        "***Load data***\n",
        "\n",
        "Although we can find MNIST dataset from Yann LeCun's official site, I chose a more convenient way to find the dataset from Keras.  \n",
        "Also, from the code below, we can show that the MNIST database contains 60,000 training and 10,000 testing images, which have $28\\times28$ pixels with only greyscale."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WnifVp2KNDGi",
        "colab_type": "code",
        "outputId": "f1ad99aa-e87e-442a-a81f-f18d4ee549d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "(train_data_ori, train_label), (test_data_ori, test_label) = mnist.load_data()\n",
        "print (\"mnist data loaded\")\n",
        "print (\"original training data shape:\",train_data_ori.shape)\n",
        "print (\"original testing data shape:\",test_data_ori.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 1s 0us/step\n",
            "mnist data loaded\n",
            "original training data shape: (60000, 28, 28)\n",
            "original testing data shape: (10000, 28, 28)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xk1TTt_m7UrD",
        "colab_type": "text"
      },
      "source": [
        "For the convenience of training, linearize each image from $28\\times28$ into an array of size $1\\times784$, so that the training and test datasets are converted into 2-dimensional vectors of size $60000\\times784$ and $10000\\times784$, respectively."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6OWqBntQgn1",
        "colab_type": "code",
        "outputId": "ed51f7af-3295-4831-e92b-7da54810f35d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "train_data=train_data_ori.reshape(60000,784)\n",
        "test_data=test_data_ori.reshape(10000,784)\n",
        "print (\"training data shape after reshape:\",train_data.shape)\n",
        "print (\"testing data shape after reshape:\",test_data.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training data shape after reshape: (60000, 784)\n",
            "testing data shape after reshape: (10000, 784)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kVv6r27Q9ztu",
        "colab_type": "text"
      },
      "source": [
        "***Dimension Reduction using PCA***  \n",
        "\n",
        "For this case, the pixels of the image will be the features used to build our predictive model. In this way, implementing KNN clustering is to calculate the norms in a 784-dimensional space.  \n",
        "However, calculating norms in this 784-dimensional space is far from easy and efficient. Intuitively, we can perform some dimention reduction before going to KNN and calculate those norms, so that we can become more efficient.  \n",
        "The way to do dimension reduction here is PCA mentioned in the lecture. I don't dig deep into PCA here, and use APIs from sklearn to implement PCA instead. I reduce the feature space from 784 dimensions into 100 dimensions. Talk is cheap, here's the code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OkARp-d0f_BZ",
        "colab_type": "code",
        "outputId": "7919fdc6-0210-4e47-ddb6-93a2099e4bb0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "pca = PCA(n_components = 100)\n",
        "pca.fit(train_data) #fit PCA with training data instead of the whole dataset\n",
        "train_data_pca = pca.transform(train_data)\n",
        "test_data_pca = pca.transform(test_data)\n",
        "print(\"PCA completed with 100 components\")\n",
        "print (\"training data shape after PCA:\",train_data_pca.shape)\n",
        "print (\"testing data shape after PCA:\",test_data_pca.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PCA completed with 100 components\n",
            "training data shape after PCA: (60000, 100)\n",
            "testing data shape after PCA: (10000, 100)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16vjYwp4GZ0S",
        "colab_type": "text"
      },
      "source": [
        "From the result above, we can know that the training and test datasets become two vectors of size $60000\\times100$ and $10000\\times100$, respectively.  \n",
        "\n",
        "At this point, the datasets are ready."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oD2TydRHYS2",
        "colab_type": "text"
      },
      "source": [
        "***Code up KNN***  \n",
        "Here's the code to K Nearest Neighbor clustering algorithm. This function takes in the image to cluster, training dataset, training labels, the value of K and the sort of norm to calculate distance(*i.e.* the value of $p$ in $l_p$ norm).  \n",
        "Under the most circumstance, we use Euclidean norm to calculate distace, thus $p=2$.  \n",
        "This function returns the class most common among the test data's K nearest neighbors, where K is the parameter mentioned above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XyHqEcjMhzTh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def KNN(test_data1,train_data_pca,train_label,k,p):\n",
        "    subMat = train_data_pca - np.tile(test_data1,(60000,1))\n",
        "    subMat = np.abs(subMat)\n",
        "    distance = subMat**p\n",
        "    distance = np.sum(distance,axis=1)\n",
        "    distance = distance**(1.0/p)\n",
        "    distanceIndex = np.argsort(distance)\n",
        "    classCount = [0,0,0,0,0,0,0,0,0,0]\n",
        "    for i in range(k):\n",
        "        label = train_label[distanceIndex[i]]\n",
        "        classCount[label] = classCount[label] + 1    \n",
        "    return np.argmax(classCount)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AgFwAuQLMmCg",
        "colab_type": "text"
      },
      "source": [
        "***Define the test function***  \n",
        "This function takes in the value of K and the value of $p$ in $l_p$ norm mentioned above, and returns the accuracy of KNN clustering on the test dataset, along with the confusion matrix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mKtjXPW2Mk1I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test(k,p):\n",
        "    print(\"testing with K= %d and lp norm p=%d\"%(k,p))\n",
        "    m,n = np.shape(test_data_pca)\n",
        "    correctCount = 0\n",
        "    M = np.zeros((10,10),int)\n",
        "    for i in range(m):\n",
        "        test_data1 = test_data_pca[i,:]\n",
        "        predict_label = KNN(test_data1,train_data_pca,train_label, k, p)\n",
        "        true_label = test_label[i]\n",
        "        M[true_label][predict_label] += 1\n",
        "#        print(\"predictï¼š%d,true:%d\" % (predict_label,true_label))\n",
        "        if true_label == predict_label:\n",
        "           correctCount += 1\n",
        "                  \n",
        "    print(\"The accuracy is: %f\" % (float(correctCount)/m))\n",
        "    print(\"Confusion matrix:\",M)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "brI_nbJGNe1n",
        "colab_type": "text"
      },
      "source": [
        "***Test result***  \n",
        "Here's the precision of the KNN clustering algorithm with argument K=3 and Euclidean norm, along with the confusion matrix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfA40lTnSDKJ",
        "colab_type": "code",
        "outputId": "416cc113-dbdc-4f02-e03f-3945a7fc524c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        }
      },
      "source": [
        "test(3,2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "testing with K= 3 and lp norm p=2\n",
            "The accuracy is: 0.973500\n",
            "Confusion matrix: [[ 974    1    1    0    0    1    2    1    0    0]\n",
            " [   0 1131    3    0    0    0    1    0    0    0]\n",
            " [   7    4 1004    1    1    0    0   13    2    0]\n",
            " [   1    1    4  979    1    9    0    7    5    3]\n",
            " [   2    5    0    0  949    0    4    3    0   19]\n",
            " [   4    1    0   10    2  865    3    1    2    4]\n",
            " [   4    3    0    0    2    4  945    0    0    0]\n",
            " [   0   17    6    0    2    0    0  996    0    7]\n",
            " [   5    1    4   17    5   10    5    3  921    3]\n",
            " [   5    5    2    8    8    2    1    6    1  971]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzd18KY5xDAZ",
        "colab_type": "text"
      },
      "source": [
        "From the result above, we can show that we achieved the precision of 0.972900, with dimension reduction (using PCA) and KNN clustering."
      ]
    }
  ]
}